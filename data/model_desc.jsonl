{"tag": "image-classification", "id": "google/vit-base-patch16-224", "desc": "Vision Transformer (base-sized model)\r\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\r\n\r\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\r\n\r\nModel description\r\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\r\n\r\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\r\n\r\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.", "inference_type": "huggingface"}
{"tag": "image-classification", "id": "Ahmed9275/Vit-Cifar100",  "desc": "This model is a fine-tuned version of google/vit-base-patch16-224-in21k on the Cifar100 dataset. It achieves the following results on the evaluation set:\r\n\r\nLoss: 0.4420\r\nAccuracy: 0.8985", "inference_type": "local"}
{"tag": "image-classification", "id": "nateraw/vit-base-patch16-224-cifar10", "metadata": {"id2label": {"LABEL_0": "airplane", "LABEL_1": "automobile", "LABEL_2": "bird", "LABEL_3": "cat", "LABEL_4": "deer", "LABEL_5": "dog", "LABEL_6": "frog", "LABEL_7": "horse", "LABEL_8": "ship", "LABEL_9": "truck"}}, "desc": "Vision Transformer Fine Tuned on CIFAR10\r\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) and fine-tuned on CIFAR10 at resolution 224x224.\r\n\r\nCheck out the code at my my Github repo.\r\n\r\nUsage\r\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\r\nfrom PIL import Image\r\nimport requests\r\n\r\nurl = 'https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png'\r\nimage = Image.open(requests.get(url, stream=True).raw)\r\nfeature_extractor = ViTFeatureExtractor.from_pretrained('nateraw/vit-base-patch16-224-cifar10')\r\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-base-patch16-224-cifar10')\r\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\r\noutputs = model(**inputs)\r\npreds = outputs.logits.argmax(dim=1)\r\n\r\nclasses = [\r\n    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\r\n]\r\nclasses[preds[0]]\r\n\r\nModel description\r\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).\r\n\r\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.", "inference_type": "local"}
{"tag": "image-classification", "id": "farleyknight/mnist-digit-classification-2022-09-04", "desc": "MNIST digit classification model based on ViT architecture, trained on grayscale images (28x28). Optimized for recognizing handwritten digits from standard MNIST-like datasets.", "inference_type": "local"}
{"tag": "image-classification", "id": "prithivMLmods/Mnist-Digits-SigLIP2", "desc": "MNIST digit classification model based on SigLIP2 architecture, trained on RGB images converted from MNIST. Suitable for recognizing handwritten digits from both grayscale and color image sources.", "inference_type": "local"}
{"tag": "image-classification", "id": "nateraw/food", "desc": "This model is a fine-tuned version of google/vit-base-patch16-224-in21k on the nateraw/food101 dataset.", "inference_type": "huggingface"}
{"tag": "image-classification", "id": "wesleyacheng/dog-breeds-multiclass-image-classification-with-vit", "desc": "This model is finetuned using the Google Vision Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to classify dog images into 120 types of dog breeds.", "inference_type": "huggingface"}
{"tag": "image-classification", "id": "playrobin/furniture-styles", "desc": "This model is trained to classify furniture images into categories.", "inference_type": "huggingface"}
{"tag": "image-classification", "id": "dima806/facial_emotions_image_detection", "desc": "This model is a model for facial emotions image detection.", "inference_type": "huggingface"}
{"tag": "image-classification", "id": "semihdervis/cat-emotion-classifier", "desc": "The model is trained to classify emotion for a cat.", "inference_type": "huggingface"}
{"tag": "image-classification", "id": "jazzmacedo/fruits-and-vegetables-detector-36", "desc": "This Model was trained with a very small dataset kritikseth/fruit-and-vegetable-image-recognition that contains only 36 labels", "inference_type": "huggingface"}
{"tag": "image-classification", "id": "dima806/67_cat_breeds_image_detection", "desc": "model is a fine-tuned version of google/vit-base-patch16-224-in21k on the catbreed dataset.", "inference_type": "local"}
{"tag": "image-classification", "id": "dima806/man_woman_face_image_detection", "desc": "Returns with about 98.7% accuracy whether the face belongs to man or woman based on face image.", "inference_type": "local"}
{"tag": "image-classification", "id": "Anwarkh1/Skin_Cancer-Image_Classification", "desc": "This model is designed for the classification of skin cancer images into various categories including benign keratosis-like lesions, basal cell carcinoma, actinic keratoses, vascular lesions, melanocytic nevi, melanoma, and dermatofibroma.", "inference_type": "huggingface"}
{"tag": "image-classification", "id": "linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification", "desc": "For identifying common diseases in crops and assessing plant health. Not to be used as a replacement for an actual diagnosis from experts. The plant village dataset consists of 38 classes of diseases in common crops (including healthy/normal crops).", "inference_type": "huggingface"}
{"tag": "image-classification", "id": "umm-maybe/AI-image-detector", "desc": "This model is a proof-of-concept demonstration of using a ViT model to predict whether an artistic image was generated using AI.", "inference_type": "huggingface"}
{"tag": "image-classification", "id": "dima806/fairface_age_image_detection", "desc": "Detects age group with about 59% accuracy based on an image.", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "ilsilfverskiold/classify-news-category-iptc", "desc": "The model is intended to categorize Norwegian, Swedish and English news content within the specified 16 categories but is a test model for demonstration purposes. It needs more data within several categories to provide 100% value but it will outperform Claude Haiku and GPT-3.5 on this use case.", "inference_type": "local"}
{"tag": "token-classification", "id": "iiiorg/piiranha-v1-detect-personal-information", "desc": "Model Description\r\nPiiranha is a fine-tuned version of microsoft/mdeberta-v3-base. The context length is 256 Deberta tokens. If your text is longer than that, just split it up.\r\n\r\nSupported languages: English, Spanish, French, German, Italian, Dutch\r\n\r\nSupported PII types: Account Number, Building Number, City, Credit Card Number, Date of Birth, Driver's License, Email, First Name, Last Name, ID Card, Password, Social Security Number, Street Address, Tax Number, Phone Number, Username, Zipcode.\r\n\r\nIt achieves the following results on a test set of ~73,000 sentences containing PII:\r\n\r\nAccuracy: 99.44%\r\nLoss: 0.0173\r\nPrecision: 93.16%\r\nRecall: 93.08%\r\nF1: 93.12%\r\nNote that the above metrics factor in the eighteen possible categories (17 PII and 1 Non PII), so the metrics are lower than the metrics for just PII vs. Non PII (binary classification).", "inference_type": "huggingface"}
{"tag": "token-classification", "id": "dslim/bert-base-NER", "desc": "Model description\r\nbert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\r\n\r\nSpecifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.\r\n\r\nIf you'd like to use a larger BERT-large model fine-tuned on the same dataset, a bert-large-NER version is also available.", "inference_type": "huggingface"}
{"tag": "token-classification", "id": "blaze999/Medical-NER", "desc": "Medical NER Model finetuned on BERT to recognize 41 Medical entities.", "inference_type": "huggingface"}
{"tag": "token-classification", "id": "w11wo/indonesian-roberta-base-posp-tagger", "desc": "This model is a fine-tuned version of flax-community/indonesian-roberta-base on the POSP (Part-of-Speech Tagging) dataset from the IndoNLU benchmark. It is designed to perform token-level classification, assigning POS tags (e.g., nouns, verbs, adjectives) to each token in Indonesian text. With high performance metrics—achieving over 96% accuracy and F1-score on the POSP benchmark—the model is well-suited for various Indonesian NLP applications, including syntactic parsing, text preprocessing, and downstream tasks like information extraction.", "inference_type": "local"}
{"tag": "text-classification", "id": "SamLowe/roberta-base-go_emotions", "desc": "Overview\r\nModel trained from roberta-base on the go_emotions dataset for multi-label classification.\r\n\r\nONNX version also available\r\nA version of this model in ONNX format (including an INT8 quantized ONNX version) is now available at https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx. These are faster for inference, esp for smaller batch sizes, massively reduce the size of the dependencies required for inference, make inference of the model more multi-platform, and in the case of the quantized version reduce the model file/download size by 75% whilst retaining almost all the accuracy if you only need inference.\r\n\r\nDataset used for the model\r\ngo_emotions is based on Reddit data and has 28 labels. It is a multi-label dataset where one or multiple labels may apply for any given input text, hence this model is a multi-label classification model with 28 'probability' float outputs for any given input text. Typically a threshold of 0.5 is applied to the probabilities for the prediction for each label.", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "rjac/bert-20news-classification", "desc": "bert-20news-classification\r\nThis model is a fine-tuned version of distilbert-base-uncased on an unknown dataset. It achieves the following results on the evaluation set:\r\n\r\nTrain Loss: 0.0479\r\nTrain Accuracy: 0.9922\r\nValidation Loss: 0.2769\r\nValidation Accuracy: 0.9284\r\nEpoch: 9\r\nModel description\r\nThis model is a fine-tuned version of the DistilBERT model for sequence classification tasks. It was trained using Hugging Face's transformers and TensorFlow. The model expects input sequences to be tokenized according to the DistilBERT's tokenizer.\r\n\r\nThe model was trained specifically for classifying text into 20 different categories derived from the 20 Newsgroups dataset. These categories include various topics such as 'alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'.\r\n\r\nIntended uses & limitations\r\nThis model is intended for classifying text into the above mentioned 20 categories. It can be used for categorizing text data from similar domains or topics.", "inference_type": "local"}
{"tag": "text-classification", "id": "mshenoda/roberta-spam", "desc": "BERT model finetuned for SMS spam detection.", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "jy46604790/Fake-News-Bert-Detect", "metadata": {"id2label": {"LABEL_0": "fake", "LABEL_1": "real"}}, "desc": "This model is trained by over 40,000 news from different medias based on the 'roberta-base'. It can give result by simply entering the text of the news less than 500 words(the excess will be truncated automatically).", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "ProsusAI/finbert", "meta": {"language": "en", "tags": ["financial-sentiment-analysis", "sentiment-analysis"], "widget": [{"text": "Stocks rallied and the British pound gained."}]}, "desc": "BERT model finetuned on Financial PhraseBank for financial sentiment analysis (positive/negative/neutral)", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "j-hartmann/emotion-english-distilroberta-base", "desc": "Description ℹ\r\nWith this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:\r\n\r\nanger 🤬\r\ndisgust 🤢\r\nfear 😨\r\njoy 😀\r\nneutral 😐\r\nsadness 😭\r\nsurprise 😲\r\nThe model is a fine-tuned checkpoint of DistilRoBERTa-base. For a 'non-distilled' emotion model, please refer to the model card of the RoBERTa-large version.", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "cardiffnlp/tweet-topic-21-multi", "desc": "This model is based on a TimeLMs language model trained on ~124M tweets from January 2018 to December 2021 (see here), and finetuned for multi-label topic classification on a corpus of 11,267 tweets. This model is suitable for English.", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "s-nlp/roberta_toxicity_classifier", "desc": "This model is trained for toxicity classification task. The dataset used for training is the merge of the English parts of the three datasets by Jigsaw (Jigsaw 2018, Jigsaw 2019, Jigsaw 2020), containing around 2 million examples. ", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "Falconsai/intent_classification", "desc":"Model Description\r\nThe Fine-Tuned DistilBERT is a variant of the BERT transformer model, distilled for efficient performance while maintaining high accuracy. It has been adapted and fine-tuned for the specific task of classifying user intent in text data.\r\n\r\nThe model, named \"distilbert-base-uncased,\" is pre-trained on a substantial amount of text data, which allows it to capture semantic nuances and contextual information present in natural language text. It has been fine-tuned with meticulous attention to hyperparameter settings, including batch size and learning rate, to ensure optimal model performance for the user intent classification task.\r\n\r\nDuring the fine-tuning process, a batch size of 8 for efficient computation and learning was chosen. Additionally, a learning rate (2e-5) was selected to strike a balance between rapid convergence and steady optimization, ensuring the model not only learns quickly but also steadily refines its capabilities throughout training.\r\n\r\nThis model has been trained on a rather small dataset of under 50k, 100 epochs, specifically designed for user intent classification. The dataset consists of text samples, each labeled with different user intents, such as \"information seeking,\" \"question asking,\" or \"opinion expressing.\" The diversity within the dataset allowed the model to learn to identify user intent accurately. This dataset was carefully curated from a variety of sources.\r\n\r\nThe goal of this meticulous training process is to equip the model with the ability to classify user intent in text data effectively, making it ready to contribute to a wide range of applications involving user interaction analysis and personalization.\r\n\r\nIntended Uses & Limitations\r\nIntended Uses\r\nUser Intent Classification: The primary intended use of this model is to classify user intent in text data. It is well-suited for applications that involve understanding user intentions, such as chatbots, virtual assistants, and recommendation systems.", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "unitary/toxic-bert", "desc": "🙊 Detoxify\r\nToxic Comment Classification with ⚡ Pytorch Lightning and 🤗 Transformers\r\nCI testing Lint\r\n\r\nExamples image\r\n\r\nDescription\r\nTrained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended Bias in Toxic comments, Multilingual toxic comment classification.\r\n\r\nBuilt by Laura Hanu at Unitary, where we are working to stop harmful content online by interpreting visual content in context.\r\n\r\nDependencies:\r\n\r\nFor inference:\r\n🤗 Transformers\r\n⚡ Pytorch lightning\r\nFor training will also need:\r\nKaggle API (to download data)\r\nChallenge\tYear\tGoal\tOriginal Data Source\tDetoxify Model Name\tTop Kaggle Leaderboard Score\tDetoxify Score\r\nToxic Comment Classification Challenge\t2018\tbuild a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.\tWikipedia Comments\toriginal\t0.98856\t0.98636\r\nJigsaw Unintended Bias in Toxicity Classification\t2019\tbuild a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias.\tCivil Comments\tunbiased\t0.94734\t0.93639\r\nJigsaw Multilingual Toxic Comment Classification\t2020\tbuild effective multilingual models\tWikipedia Comments + Civil Comments\tmultilingual\t0.9536\t0.91655*\r\n*Score not directly comparable since it is obtained on the validation set provided and not on the test set. To update when the test labels are made available.\r\n\r\nIt is also noteworthy to mention that the top leadearboard scores have been achieved using model ensembles. The purpose of this library was to build something user-friendly and straightforward to use.\r\n\r\nLimitations and ethical considerations\r\nIf words that are associated with swearing, insults or profanity are present in a comment, it is likely that it will be classified as toxic, regardless of the tone or the intent of the author e.g. humorous/self-deprecating. This could present some biases towards already vulnerable minority groups.\r\n\r\nThe intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics and/or to aid content moderators in flagging out harmful content quicker.\r\n\r\nSome useful resources about the risk of different biases in toxicity or hate speech detection are:\r\n\r\nThe Risk of Racial Bias in Hate Speech Detection\r\nAutomated Hate Speech Detection and the Problem of Offensive Language\r\nRacial Bias in Hate Speech and Abusive Language Detection Datasets", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "vineetsharma/customer-support-intent-albert", "desc": "This model is a fine-tuned version of albert-base-v2 for intent classification on the bitext/Bitext-customer-support-llm-chatbot-training-dataset dataset. specialized in classifying customer intentions in customer support and care situations. It helps to quickly and accurately determine the customer's purpose when interacting with the support system, thereby improving service efficiency and customer experience.", "inference_type": "huggingface"}
{"tag": "text-classification", "id": "ElKulako/cryptobert", "desc": "CryptoBERT is a pre-trained NLP model to analyse the language and sentiments of cryptocurrency-related social media posts and messages. It was built by further training the vinai's bertweet-base language model on the cryptocurrency domain, using a corpus of over 3.2M unique cryptocurrency-related social media posts. The model was trained on the following labels: 'Bearish' : 0, 'Neutral': 1, 'Bullish': 2. CryptoBERT's sentiment classification head was fine-tuned on a balanced dataset of 2M labelled StockTwits posts, sampled from ElKulako/stocktwits-crypto. CryptoBERT was trained with a max sequence length of 128. Technically, it can handle sequences of up to 514 tokens, however, going beyond 128 is not recommended.", "inference_type": "local"}
{"tag": "text-classification", "id": "thanhtlx/text_classification_2", "desc": "Institution Name Normalization Model\r\n📝 Mô tả\r\nĐây là một mô hình phân loại văn bản được huấn luyện để chuẩn hóa các tên cơ quan (institutions) được viết theo nhiều cách khác nhau trong các bài báo khoa học. Mô hình nhận diện và ánh xạ các tên cơ quan (có thể khác nhau về ngôn ngữ, cách viết tắt, hoặc cách trình bày) thành một tên chuẩn hóa duy nhất.\r\n\r\nVí dụ:\r\n\r\n\"VNU University of Engineering and Technology\" → uet\r\n\"University of Science, Vietnam National University Hanoi\" → hus\r\nMô hình sử dụng kiến trúc BERT và được huấn luyện trên tập dữ liệu chứa các tên cơ quan từ các bài báo khoa học.\r\n\r\n📌 Chi tiết các mô hình\r\nTên model\tKiến trúc\tChức năng chính\r\nthanhtlx/text_classification_2\tRobertaForSequenceClassification\tChuẩn hóa tên cơ quan thành một trong 12 nhãn chuẩn\r\n📥 Đầu vào\r\nĐịnh dạng: Tên cơ quan dưới dạng chuỗi văn bản (có thể bằng tiếng Anh, tiếng Việt, hoặc viết tắt)\r\nKiểu dữ liệu: Chuỗi văn bản (str)\r\nXử lý: Văn bản sẽ được mã hóa bởi tokenizer của mô hình (tự động cắt ngắn và đệm nếu cần)\r\nVí dụ đầu vào:\r\n\r\n\"VNU University of Engineering and Technology\"\r\n\"ĐH Bách Khoa Hà Nội\"\r\n\"University of Science, Vietnam National University Hanoi\"\r\n📤 Đầu ra\r\nMột chuỗi ký hiệu tương ứng với tên cơ quan chuẩn hóa:\r\nsmp: School of Medicine and Pharmacy\r\nueb: University of Economics and Business\r\nussh: University of Social Sciences and Humanities\r\nhus: University of Science\r\nhsb: Hanoi School of Business\r\nlaw: School of Law\r\nulis: University of Languages and International Studies\r\nvju: Vietnam Japan University\r\nuet: University of Engineering and Technology\r\nued: University of Education\r\nis: International School\r\niti: Information Technology Institute\r\nother: Others\r\nVí dụ:\r\n\r\n\"VNU University of Engineering and Technology\" → uet\r\n\"University of Science, Vietnam National University Hanoi\" → hus\r\n🧪 Sử dụng mô hình Dưới đây là đoạn mã mẫu để sử dụng mô hình:\r\n\r\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\r\nimport tensorflow as tf\r\n\r\n# Tải tokenizer và mô hình\r\ntokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_2\")\r\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_2\")\r\n\r\n# Tên cơ quan cần chuẩn hóa\r\ntext = \"VNU University of Engineering and Technology\"\r\n\r\n# Mã hóa văn bản\r\ninputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\r\n\r\n# Dự đoán\r\noutputs = model(**inputs)\r\nlogits = outputs.logits\r\n\r\n# Chuyển logits thành xác suất\r\nprobs = tf.nn.softmax(logits, axis=1)\r\n\r\n# Lấy nhãn có xác suất cao nhất\r\npredicted_label = tf.argmax(probs, axis=1).numpy()[0]\r\nprint(f\"Nhãn dự đoán: {model.config.id2label[predicted_label]}\")", "inference_type": "local"}
{"tag": "text-classification", "id": "papluca/xlm-roberta-base-language-detection", "desc": "This is an XLM-RoBERTa-base model fine-tuned for language detection.[1] It supports 20 languages and can be used directly for classification tasks.[1] The model achieves an average accuracy of 99.6% on its test data.", "inference_type": "local"}
{"tag": "video-classification", "id": "MCG-NJU/videomae-base-finetuned-kinetics", "desc": "Model finetuned on Kinetics dataset for video classification.", "inference_type": "local"}
{"tag": "audio-classification", "id": "MIT/ast-finetuned-audioset-10-10-0.4593", "desc": "AST model finetuned on AudioSet for audio classification.", "inference_type": "local"}
{"tag": "audio-classification", "id": "firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3", "desc": "wav2vec2-base-Speech_Emotion_Recognition\r\nThis model is a fine-tuned version of facebook/wav2vec2-base.\r\n\r\nIt achieves the following results on the evaluation set:\r\n\r\nLoss: 0.7264\r\nAccuracy: 0.7539\r\nF1\r\nWeighted: 0.7514\r\nMicro: 0.7539\r\nMacro: 0.7529\r\nRecall\r\nWeighted: 0.7539\r\nMicro: 0.7539\r\nMacro: 0.7577\r\nPrecision\r\nWeighted: 0.7565\r\nMicro: 0.7539\r\nMacro: 0.7558\r\nModel description\r\nThis model predicts the emotion of the person speaking in the audio sample.\r\n\r\nFor more information on how it was created, check out the following link: https://github.com/DunnBC22/Vision_Audio_and_Multimodal_Projects/tree/main/Audio-Projects/Emotion%20Detection/Speech%20Emotion%20Detection\r\n\r\nIntended uses & limitations\r\nThis model is intended to demonstrate my ability to solve a complex problem using technology.", "inference_type": "local"}
{"tag": "audio-classification", "id": "DunnBC22/wav2vec2-base-Speech_Emotion_Recognition", "desc":"wav2vec2-base-Speech_Emotion_Recognition\r\nThis model is a fine-tuned version of facebook/wav2vec2-base.\r\n\r\nIt achieves the following results on the evaluation set:\r\n\r\nLoss: 0.7264\r\nAccuracy: 0.7539\r\nF1\r\nWeighted: 0.7514\r\nMicro: 0.7539\r\nMacro: 0.7529\r\nRecall\r\nWeighted: 0.7539\r\nMicro: 0.7539\r\nMacro: 0.7577\r\nPrecision\r\nWeighted: 0.7565\r\nMicro: 0.7539\r\nMacro: 0.7558\r\nModel description\r\nThis model predicts the emotion of the person speaking in the audio sample.\r\n\r\nFor more information on how it was created, check out the following link: https://github.com/DunnBC22/Vision_Audio_and_Multimodal_Projects/tree/main/Audio-Projects/Emotion%20Detection/Speech%20Emotion%20Detection\r\n\r\nIntended uses & limitations\r\nThis model is intended to demonstrate my ability to solve a complex problem using technology.\r\n\r\nTraining and evaluation data\r\nDataset Source: https://www.kaggle.com/datasets/dmitrybabko/speech-emotion-recognition-en\r\n\r\nTraining procedure\r\nTraining hyperparameters\r\nThe following hyperparameters were used during training:\r\n\r\nlearning_rate: 3e-05\r\ntrain_batch_size: 32\r\neval_batch_size: 32\r\nseed: 42\r\ngradient_accumulation_steps: 4\r\ntotal_train_batch_size: 128\r\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\r\nlr_scheduler_type: linear\r\nlr_scheduler_warmup_ratio: 0.1\r\nnum_epochs: 10\r\nTraining results\r\nTraining Loss\tEpoch\tStep\tValidation Loss\tAccuracy\tWeighted F1\tMicro F1\tMacro F1\tWeighted Recall\tMicro Recall\tMacro Recall\tWeighted Precision\tMicro Precision\tMacro Precision\r\n1.5581\t0.98\t43\t1.4046\t0.4653\t0.4080\t0.4653\t0.4174\t0.4653\t0.4653\t0.4793\t0.5008\t0.4653\t0.4974\r\n1.5581\t1.98\t86\t1.1566\t0.5997\t0.5836\t0.5997\t0.5871\t0.5997\t0.5997\t0.6093\t0.6248\t0.5997\t0.6209\r\n1.5581\t2.98\t129\t0.9733\t0.6883\t0.6845\t0.6883\t0.6860\t0.6883\t0.6883\t0.6923\t0.7012\t0.6883\t0.7009\r\n1.5581\t3.98\t172\t0.8313\t0.7399\t0.7392\t0.7399\t0.7409\t0.7399\t0.7399\t0.7417\t0.7415\t0.7399\t0.7432\r\n1.5581\t4.98\t215\t0.8708\t0.7028\t0.6963\t0.7028\t0.6970\t0.7028\t0.7028\t0.7081\t0.7148\t0.7028\t0.7114\r\n1.5581\t5.98\t258\t0.7969\t0.7297\t0.7267\t0.7297\t0.7277\t0.7297\t0.7297\t0.7333\t0.7393\t0.7297\t0.7382\r\n1.5581\t6.98\t301\t0.7349\t0.7603\t0.7613\t0.7603\t0.7631\t0.7603\t0.7603\t0.7635\t0.7699\t0.7603\t0.7702\r\n1.5581\t7.98\t344\t0.7714\t0.7469\t0.7444\t0.7469\t0.7456\t0.7469\t0.7469\t0.7485\t0.7554\t0.7469\t0.7563\r\n1.5581\t8.98\t387\t0.7183\t0.7630\t0.7615\t0.7630\t0.7631\t0.7630\t0.7630\t0.7652\t0.7626\t0.7630\t0.7637\r\n1.5581\t9.98\t430\t0.7264\t0.7539\t0.7514\t0.7539\t0.7529\t0.7539\t0.7539\t0.7577\t0.", "inference_type": "local"}
{"tag": "audio-classification", "id": "dima806/music_genres_classification", "desc": "Music genre classification is a fundamental and versatile application in many various domains. Some possible use cases for music genre classification include:\r\n\r\nmusic recommendation systems;\r\ncontent organization and discovery;\r\nradio broadcasting and programming;\r\nmusic licensing and copyright management;\r\nmusic analysis and research;\r\ncontent tagging and metadata enrichment;\r\naudio identification and copyright protection;\r\nmusic production and creativity;\r\nhealthcare and therapy;\r\nentertainment and gaming.\r\nThe model is trained based on publicly available dataset of labeled music data — GTZAN Dataset — that contains 1000 sample 30-second audio files evenly split among 10 genres:\r\n\r\nblues;\r\nclassical;\r\ncountry;\r\ndisco;\r\nhip-hop;\r\njazz;\r\nmetal;\r\npop;\r\nreggae;\r\nrock.", "inference_type": "local"}
{"tag": "audio-classification", "id": "anton-l/wav2vec2-base-lang-id", "desc": "This model is a fine-tuned version of facebook/wav2vec2-base on the anton-l/common_language dataset. ", "inference_type": "local"}
{"tag": "translation", "id": "Helsinki-NLP/opus-mt-fr-en", "desc": "Opus-mt model finetuned on French-English dataset for translation.", "inference_type": "huggingface"}
{"tag": "translation", "id": "Helsinki-NLP/opus-mt-en-fr", "desc": "Opus-mt model finetuned on English-French dataset for translation.", "inference_type": "huggingface"}
{"tag": "translation", "id": "Helsinki-NLP/opus-mt-en-zh", "desc": "Opus-mt model finetuned on English-Chinese dataset for translation.", "inference_type": "huggingface"}
{"tag": "translation", "id": "Helsinki-NLP/opus-mt-zh-en", "desc": "Opus-mt model finetuned on Chinese-English dataset for translation.", "inference_type": "huggingface"}
{"tag": "translation", "id": "Helsinki-NLP/opus-mt-en-es", "desc": "Opus-mt model finetuned on English-Spanish dataset for translation.", "inference_type": "huggingface"}
{"tag": "translation", "id": "utrobinmv/t5_translate_en_ru_zh_large_1024", "desc": "T5 English, Russian and Chinese multilingual machine translation\r\nThis model represents a conventional T5 transformer in multitasking mode for translation into the required language, precisely configured for machine translation for pairs: ru-zh, zh-ru, en-zh, zh-en, en-ru, ru-en.\r\n\r\nThe model can perform direct translation between any pair of Russian, Chinese or English languages. For translation into the target language, the target language identifier is specified as a prefix 'translate to :'. In this case, the source language may not be specified, in addition, the source text may be multilingual.\r\n\r\nExample translate Russian to Chinese\r\n\r\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\r\n\r\nmodel_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\r\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\r\ntokenizer = T5Tokenizer.from_pretrained(model_name)\r\n\r\nprefix = 'translate to zh: '\r\nsrc_text = prefix + \"Цель разработки — предоставить пользователям личного синхронного переводчика.\"\r\n\r\n# translate Russian to Chinese\r\ninput_ids = tokenizer(src_text, return_tensors=\"pt\")\r\n\r\ngenerated_tokens = model.generate(**input_ids)\r\n\r\nresult = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\r\nprint(result)\r\n#开发的目的是为用户提供个人同步翻译。\r\n\r\nand Example translate Chinese to Russian\r\n\r\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\r\n\r\nmodel_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\r\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\r\ntokenizer = T5Tokenizer.from_pretrained(model_name)\r\n\r\nprefix = 'translate to ru: '\r\nsrc_text = prefix + \"开发的目的是为用户提供个人同步翻译。\"\r\n\r\n# translate Russian to Chinese\r\ninput_ids = tokenizer(src_text, return_tensors=\"pt\")\r\n\r\ngenerated_tokens = model.generate(**input_ids)\r\n\r\nresult = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\r\nprint(result)\r\n#Цель разработки - предоставить пользователям персональный синхронный перевод.", "inference_type": "huggingface"}
{"tag": "translation", "id": "google-t5/t5-large", "desc": "The developers of the Text-To-Text Transfer Transformer (T5) write:\r\n\r\nWith T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\r\n\r\nT5-Large is the checkpoint with 770 million parameters.\r\n\r\nDeveloped by: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See associated paper and GitHub repo\r\nModel type: Language model\r\nLanguage(s) (NLP): English, French, Romanian, German\r\nLicense: Apache 2.0\r\nRelated Models: All T5 Checkpoints\r\nResources for more information:\r\nResearch paper\r\nGoogle's T5 Blog Post\r\nGitHub Repo\r\nHugging Face T5 Docs", "inference_type":"huggingface"}
{"tag": "translation", "id": "VietAI/envit5-translation", "desc": "EnViT5 Translation\r\nPWC\r\n\r\nPWC\r\n\r\nState-of-the-art English-Vietnamese and Vietnamese-English Translation models trained on MTet, PhoMT.\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\r\n\r\n\r\nmodel_name = \"VietAI/envit5-translation\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)  \r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\r\n\r\ninputs = [\r\n    \"vi: VietAI là tổ chức phi lợi nhuận với sứ mệnh ươm mầm tài năng về trí tuệ nhân tạo và xây dựng một cộng đồng các chuyên gia trong lĩnh vực trí tuệ nhân tạo đẳng cấp quốc tế tại Việt Nam.\",\r\n    \"vi: Theo báo cáo mới nhất của Linkedin về danh sách việc làm triển vọng với mức lương hấp dẫn năm 2020, các chức danh công việc liên quan đến AI như Chuyên gia AI (Artificial Intelligence Specialist), Kỹ sư ML (Machine Learning Engineer) đều xếp thứ hạng cao.\",\r\n    \"en: Our teams aspire to make discoveries that impact everyone, and core to our approach is sharing our research and tools to fuel progress in the field.\",\r\n    \"en: We're on a journey to advance and democratize artificial intelligence through open source and open science.\"\r\n    ]\r\n\r\noutputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids.to('cuda'), max_length=512)\r\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\r\n\r\n# ['en: VietAI is a non-profit organization with the mission of nurturing artificial intelligence talents and building an international - class community of artificial intelligence experts in Vietnam.',\r\n#  'en: According to the latest LinkedIn report on the 2020 list of attractive and promising jobs, AI - related job titles such as AI Specialist, ML Engineer and ML Engineer all rank high.',\r\n#  'vi: Nhóm chúng tôi khao khát tạo ra những khám phá có ảnh hưởng đến mọi người, và cốt lõi trong cách tiếp cận của chúng tôi là chia sẻ nghiên cứu và công cụ để thúc đẩy sự tiến bộ trong lĩnh vực này.',\r\n#  'vi: Chúng ta đang trên hành trình tiến bộ và dân chủ hoá trí tuệ nhân tạo thông qua mã nguồn mở và khoa học mở.']\r\n\r\nResults\r\nimage\r\n\r\nCitation\r\n@misc{https://doi.org/10.48550/arxiv.2210.05610,\r\n  doi = {10.48550/ARXIV.2210.05610},\r\n  author = {Ngo, Chinh and Trinh, Trieu H. and Phan, Long and Tran, Hieu and Dang, Tai and Nguyen, Hieu and Nguyen, Minh and Luong, Minh-Thang},\r\n  title = {MTet: Multi-domain Translation for English and Vietnamese},\r\n  publisher = {arXiv},\r\n  year = {2022},\r\n}", "inference_type": "huggingface"}
{"tag": "summarization", "id": "human-centered-summarization/financial-summarization-pegasus", "desc": "This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies.", "inference_type": "huggingface"}
{"tag": "summarization", "id": "Falconsai/text_summarization", "desc": "Model Description\r\nThe Fine-Tuned T5 Small is a variant of the T5 transformer model, designed for the task of text summarization. It is adapted and fine-tuned to generate concise and coherent summaries of input text.\r\n\r\nThe model, named \"t5-small,\" is pre-trained on a diverse corpus of text data, enabling it to capture essential information and generate meaningful summaries. Fine-tuning is conducted with careful attention to hyperparameter settings, including batch size and learning rate, to ensure optimal performance for text summarization.\r\n\r\nDuring the fine-tuning process, a batch size of 8 is chosen for efficient computation and learning. Additionally, a learning rate of 2e-5 is selected to balance convergence speed and model optimization. This approach guarantees not only rapid learning but also continuous refinement during training.\r\n\r\nThe fine-tuning dataset consists of a variety of documents and their corresponding human-generated summaries. This diverse dataset allows the model to learn the art of creating summaries that capture the most important information while maintaining coherence and fluency.\r\n\r\nThe goal of this meticulous training process is to equip the model with the ability to generate high-quality text summaries, making it valuable for a wide range of applications involving document summarization and content condensation.", "inference_type": "huggingface"}
{"tag": "summarization", "id": "jotamunz/billsum_tiny_summarization", "desc": "This model is a fine-tuned version of  google/t5-efficient-tiny  on the billsum dataset.", "inference_type": "huggingface"}
{"tag": "summarization", "id": "google/pegasus-multi_news", "desc": "This model is trained on both C4 and HugeNews (dataset mixture is weighted by their number of examples). trained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity). the model uniformly sample a gap sentence ratio between 15% and 45%. importance sentences are sampled using a 20% uniform noise to importance scores. the sentencepiece tokenizer is updated to be able to encode newline character.", "inference_type": "huggingface"}
{"tag": "summarization", "id": "knkarthick/meeting-summary-samsum", "desc": "This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.", "inference_type": "huggingface"}
{"tag": "summarization", "id": "pszemraj/led-large-book-summary", "desc": "TThis model is a fine-tuned version of allenai/led-large-16384 on the BookSum dataset (kmfoda/booksum). It aims to generalize well and be useful in summarizing lengthy text for both academic and everyday purposes.", "inference_type": "huggingface"}
{"tag": "summarization", "id": "csebuetnlp/mT5_multilingual_XLSum", "desc": "This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset.", "inference_type": "huggingface"}
{"tag": "object-detection", "id": "Ultralytics/YOLO11", "desc": "YOLO11 model trained on COCO 2017 dataset for object detection.", "inference_type": "local"}
{"tag": "automatic-speech-recognition", "id": "openai/whisper-large-v3", "desc": "Whisper is a general-purpose speech recognition model. It is trained on a large corpus of diverse audio and is also a multi-task model that can perform multilingual speech recognition, translation, and transcription.", "inference_type": "huggingface"}
{"tag": "question-answering", "id": "deepset/roberta-base-squad2", "desc": "This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering. We have also released a distilled version of this model called deepset/tinyroberta-squad2. It has a comparable prediction quality and runs at twice the speed of deepset/roberta-base-squad2.", "inference_type": "huggingface"}
{"tag": "sentence-transformer", "id": "BAAI/bge-m3", "desc": "BGE-M3 (paper, code)\r\nIn this project, we introduce BGE-M3, which is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.\r\n\r\nMulti-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.\r\nMulti-Linguality: It can support more than 100 working languages.\r\nMulti-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens.", "inference_type": "huggingface"}
{"tag": "image-to-text", "id": "Salesforce/blip-image-captioning-base", "desc": "Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.", "inference_type": "local"}
{"tag": "image-to-text", "id": "microsoft/trocr-base-printed", "desc": "TrOCR (small-sized model, fine-tuned on SROIE)\r\nTrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.\r\n\r\nModel description\r\nThe TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.\r\n\r\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.\r\n\r\nIntended uses & limitations\r\nYou can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.\r\n\r\nHow to use\r\nHere is how to use this model in PyTorch:\r\n\r\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\r\nfrom PIL import Image\r\nimport requests\r\n\r\n# load image from the IAM database (actually this model is meant to be used on printed text)\r\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\r\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\r\n\r\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\r\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\r\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\r\n\r\ngenerated_ids = model.generate(pixel_values)\r\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\n\r\nBibTeX entry and citation info\r\n@misc{li2021trocr,\r\n      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, \r\n      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\r\n      year={2021},\r\n      eprint={2109.10282},\r\n      archivePrefix={arXiv},\r\n      primaryClass={cs.CL}\r\n}\r\n\r\nDownloads last month\r\n41,685\r\nSafetensors\r\nModel size\r\n61.4M params\r\nTensor type\r\nF32\r\n\r\nFiles info\r\n\r\nInference Providers\r\nNEW\r\nImage-to-Text\r\nThis model isn't deployed by any Inference Provider.\r\n🙋\r\n1\r\nAsk for provider support\r\nModel tree for\r\nmicrosoft/trocr-small-printed\r\nFinetunes\r\n1 model\r\nQuantizations\r\n1 model\r\nSpaces using\r\nmicrosoft/trocr-small-printed\r\n24\r\n🐠\r\nsaifulbabo/math-captcha1\r\n🏆\r\nwai572/board-recognizer\r\n👀\r\nyhshin/latex-ocr\r\n👀\r\ntomofi/trocr-captcha\r\n👀\r\nJUNGU/latex-ocr-wthGPT\r\n⚡\r\ngodlyjkrjjjcope/rettttt\r\n🚀\r\nsfwer/sdrgfergert\r\n🚀\r\npENrknSoysneed/8kun-captcha-trocr\r\n👀\r\nAbdo96/latex-ocr\r\n👀\r\ncxeep/latex-ocr\r\n👀\r\nJUNGU/latex-ocr-new\r\n🐨\r\n", "inference_type": "local"}
{"tag": "image-to-text", "id": "microsoft/trocr-base-handwritten", "desc": "TrOCR (base-sized model, fine-tuned on IAM)\r\nTrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.\r\n\r\nDisclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.\r\n\r\nModel description\r\nThe TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\r\n\r\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.\r\n\r\nIntended uses & limitations\r\nYou can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.", "inference_type": "local"}
{"tag": "image-to-text", "id": "breezedeus/pix2text-mfr", "desc": "Mathematical Formula Recognition (MFR) model from Pix2Text (P2T).\r\n\r\nModel Details / 模型细节\r\nThis MFR model utilizes the TrOCR architecture developed by Microsoft, starting with its initial values and retrained using a dataset of mathematical formula images. The resulting MFR model can be used to convert images of mathematical formulas into LaTeX text representation. More detailed can be found: Pix2Text V1.0 New Release: The Best Open-Source Formula Recognition Model | Breezedeus.com.\r\n\r\n此 MFR 模型使用了微软的 TrOCR 架构，以其为初始值并利用数学公式图片数据集进行了重新训练。 获得的 MFR 模型可用于把数学公式图片转换为 LaTeX 文本表示。更多细节请见：Pix2Text V1.0 新版发布：最好的开源公式识别模型 | Breezedeus.com。\r\n\r\nUsage and Limitations / 使用和限制\r\nPurpose: This model is a mathematical formula recognition model, capable of converting input images of mathematical formulas into LaTeX text representation.\r\n\r\nLimitation: Since the model is trained on images of mathematical formulas, it may not work when recognizing other types of images.\r\n\r\n用途：此模型为数学公式识别模型，它可以把输入的数学公式图片转换为 LaTeX 文本表示。\r\n\r\n限制：由于模型是在数学公式图片数据上训练的，它在识别其他类型的图片时可能无法工作。\r\n\r\nDocuments / 文档\r\nPix2Text V1.0 New Release: The Best Open-Source Formula Recognition Model | Breezedeus.com ;\r\nPix2Text (P2T) Github: breezedeus/pix2text ;\r\nPix2Text Online Free Service: p2t.breezedeus.com ;\r\nPix2Text Online Docs: Docs ;\r\nPix2Text More: breezedeus.com/pix2text ;\r\nPix2Text Discard: https://discord.gg/GgD87WM8Tf\r\nExamples / 示例\r\nPrinted Math Formula Images / 印刷体公式图片\r\nprinted-formula examples\r\n\r\nHandwritten Math Formula Images / 印刷体公式图片\r\nhandwritten-formula examples\r\n\r\nModel Use / 模型使用\r\nMethod 1: Using the model Directly\r\nThis method doesn't need to install pix2text, but can only recognize pure formula images.\r\n\r\n这种方法无需安装 pix2text，但只能识别纯公式图片。\r\n\r\n#! pip install transformers>=4.37.0 pillow optimum[onnxruntime]\r\nfrom PIL import Image\r\nfrom transformers import TrOCRProcessor\r\nfrom optimum.onnxruntime import ORTModelForVision2Seq\r\n\r\nprocessor = TrOCRProcessor.from_pretrained('breezedeus/pix2text-mfr')\r\nmodel = ORTModelForVision2Seq.from_pretrained('breezedeus/pix2text-mfr', use_cache=False)\r\n\r\nimage_fps = [\r\n    'examples/example.jpg',\r\n    'examples/42.png',\r\n    'examples/0000186.png',\r\n]\r\nimages = [Image.open(fp).convert('RGB') for fp in image_fps]\r\npixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\r\ngenerated_ids = model.generate(pixel_values)\r\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\r\nprint(f'generated_ids: {generated_ids}, \\ngenerated text: {generated_text}')\r\n\r\nMethod 2: Using Pix2Text\r\nThis method requires the installation of pix2text, utilizing the Mathematical Formula Detection model (MFD) within Pix2Text. It is capable of recognizing not only pure formula images but also mixed images containing text.\r\n\r\n这种方法需要安装 pix2text，借助 Pix2Text 中的数学公式检测模型（MFD），它不仅可以识别纯公式图片，还可以识别包含文本的混合图片。\r\n\r\n$ pip install pix2text>=1.1\r\n\r\n#! pip install pix2text>=1.1\r\n\r\nfrom pix2text import Pix2Text, merge_line_texts\r\n\r\nimage_fps = [\r\n    'examples/example.jpg',\r\n    'examples/42.png',\r\n    'examples/0000186.png',\r\n]\r\np2t = Pix2Text.from_config()\r\nouts = p2t.recognize_formula(image_fps)  # recognize pure formula images\r\n\r\nouts2 = p2t.recognize('examples/mixed.jpg', file_type='text_formula', return_text=True, save_analysis_res='mixed-out.jpg')  # recognize mixed images\r\nprint(outs2)\r\n\r\nMethod 3: Notebook\r\nJust try Pix2Text with this notebook: https://github.com/breezedeus/Pix2Text/blob/main/pix2text_v1_1.ipynb.\r\n\r\nPerformance / 性能\r\nThe original images for the test data are derived from real data uploaded by users on the Pix2Text Online Service. Initially, real user data from a specific period is selected, and then the Mathematical Formula Detection model (MFD) within Pix2Text is used to detect the mathematical formulas in these images and crop the corresponding parts. A subset of these formula images is then randomly chosen for manual annotation to create the test dataset. The following image shows some sample pictures from the test dataset. It is evident that the images in the test dataset are quite diverse, including mathematical formulas of various lengths and complexities, from single letters to formula groups and even matrices. This test dataset includes 485 images.\r\n\r\n测试数据对应的原始图片来源于 Pix2Text 网页版 用户上传的真实数据。首先选取一段时间内用户的真实数据，然后利用 Pix2Text 中数学公式检测模型（MFD）检测出这些图片中的数学公式并截取出对应的部分，再从中随机选取部分公式图片进行人工标注。就获得了用于测试的测试数据集了。下图是测试数据集中的部分样例图片。从中可以看出测试数据集中的图片比较多样，包括了各种不同长度和复杂度的数学公式，有单个字母的图片，也有公式组甚至矩阵图片。本测试数据集包括了 485 张图片。\r\n\r\nExamples from test data\r\n\r\nBelow are the Character Error Rates (CER, the lower, the better) of various models on this test dataset. For the true annotated results, as well as the output of each model, normalization was first performed to ensure that irrelevant factors such as spaces do not affect the test outcomes. For the recognition results of Texify, the leading and trailing symbols $ or $$ of the formula are removed first.\r\n\r\n以下是各个模型在此测试数据集上的 CER（字错误率，越小越好）。其中对真实标注结果，以及每个模型的输出都首先进行了标准化，以保证不会因为空格等无关因素影响测试结果。对 Texify 的识别结果会首先去掉公式的首尾符号$或$$。\r\n\r\nCER Comparison Among Different MFR Models\r\n\r\nAs can be seen from the figure above, the Pix2Text V1.0 MFR open-source free version model has significantly outperformed the previous versions of the paid model. Moreover, compared to the V1.0 MFR open-source free model, the precision of the Pix2Text V1.0 MFR paid model has been further improved.\r\n\r\n由上图可见，Pix2Text V1.0 MFR 开源免费版模型已经大大优于之前版本的付费模型。而相比 V1.0 MFR 开源免费模型，Pix2Text V1.0 MFR 付费模型精度得到了进一步的提升。\r\n\r\nTexify is more suited for recognizing images with standard formatting. It performs poorly in recognizing images containing single letters. This is the main reason why Texify's performance on this test dataset is inferior to that of Latex-OCR.\r\n\r\nTexify 更适用于识别标准排版的图片，它对包含单字母的图片识别较差。这也是 Texify 在此测试数据集上效果比 Latex-OCR 还差的主要原因。\r\n\r\nFeedback / 反馈\r\nWhere to send questions or comments about the model.\r\n\r\nWelcome to contact the author Breezedeus.\r\n\r\n欢迎联系作者 Breezedeus 。", "inference_type": "local"}
{"tag": "image-to-text", "id": "to-be/donut-base-finetuned-invoices", "desc": "Donut finetuned on invoices\r\nBased on Donut base model (introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository.\r\n\r\nThe model was trained with a few thousand of annotated invoices and non-invoices (for those the doctype will be 'Other'). They span across different countries and languages. They are always one page only. The dataset is proprietary unfortunately. Model is set to input resolution of 1280x1920 pixels. So any sample you want to try with higher dpi than 150 has no added value. It was trained for about 4 hours on a NVIDIA RTX A4000 for 20k steps with a val_metric of 0.03413819904382196 at the end. The following indexes were included in the train set:\r\n\r\nDocType Currency DocumentDate GrossAmount InvoiceNumber NetAmount TaxAmount OrderNumber CreditorCountry\r\n\r\nDemo space can be found here\r\n\r\nModel description\r\nDonut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\r\n\r\nmodel image\r\n\r\nIntended uses & limitations\r\nThis model is meant as a research in how well it fares with multilanguage invoices. See my observations in the demo space.\r\n\r\nHow to use\r\nLook at the documentation which includes code examples.", "inference_type": "local"}
{"tag": "tabular-classification", "id": "AWeirdDev/human-disease-prediction", "desc": "This model predicts possible human diseases based on symptom descriptions provided in natural language. It is trained on a structured dataset of symptoms and associated diseases, enabling it to classify or suggest likely medical conditions from user input.", "inference_type": "local"}
{"tag": "tabular-classification", "id": "keras-io/imbalanced_classification", "desc": "Model Description\r\nKeras Implementation of Imbalanced classification: credit card fraud detection\r\nThis repo contains the trained model of Imbalanced classification: credit card fraud detection. The full credit goes to: fchollet\r\n\r\nIntended uses & limitations\r\nThe trained model is used to detect of a specific transaction is fraudulent or not.", "inference_type": "local"}
{"tag": "tabular-regression", "id": "sadhaklal/mlp-california-housing", "desc": "mlp-california-housing\r\nA multi-layer perceptron (MLP) trained on the California Housing dataset.\r\n\r\nIt takes eight inputs: 'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude' and 'Longitude'. It predicts 'MedHouseVal'.\r\n\r\nIt is a PyTorch adaptation of the TensorFlow model in Chapter 10 of Aurelien Geron's book 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow'.\r\n\r\nCode: https://github.com/sambitmukherjee/handson-ml3-pytorch/blob/main/chapter10/mlp_california_housing.ipynb\r\n\r\nExperiment tracking: https://wandb.ai/sadhaklal/mlp-california-housing", "inference_type": "local"}
{"tag": "tabular-regression", "id": "quantile-forest/california-housing-example", "desc": "This is a RandomForestQuantileRegressor trained on the California Housing dataset. The model was trained using default parameters on a 5-fold cross-validation pipeline.", "inference_type": "local"}
{"tag": "text-generation", "id": "deepseek-ai/deepseek-coder-1.3b-base", "desc": "1. Introduction of Deepseek Coder\r\nDeepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.\r\n\r\nMassive Training Data: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\r\n\r\nHighly Flexible & Scalable: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\r\n\r\nSuperior Model Performance: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\r\n\r\nAdvanced Code Completion Capabilities: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\r\n\r\n2. Model Summary\r\ndeepseek-coder-1.3b-base is a 1.3B parameter model with Multi-Head Attention trained on 1 trillion tokens.\r\n\r\nHome Page: DeepSeek\r\nRepository: deepseek-ai/deepseek-coder\r\nChat With DeepSeek Coder: DeepSeek-Coder", "inference_type": "local"}
{"tag": "zero-shot-classification", "id": "sileod/deberta-v3-base-tasksource-nli", "desc": "Model Card for DeBERTa-v3-base-tasksource-nli\r\nNOTE\r\n\r\nDeprecated: use https://huggingface.co/tasksource/deberta-small-long-nli for longer context and better accuracy.\r\n\r\nThis is DeBERTa-v3-base fine-tuned with multi-task learning on 600+ tasks of the tasksource collection. This checkpoint has strong zero-shot validation performance on many tasks (e.g. 70% on WNLI), and can be used for:\r\n\r\nZero-shot entailment-based classification for arbitrary labels [ZS].\r\nNatural language inference [NLI]\r\nHundreds of previous tasks with tasksource-adapters [TA].\r\nFurther fine-tuning on a new task or tasksource task (classification, token classification or multiple-choice) [FT].\r\n[ZS] Zero-shot classification pipeline\r\nfrom transformers import pipeline\r\nclassifier = pipeline(\"zero-shot-classification\",model=\"sileod/deberta-v3-base-tasksource-nli\")\r\n\r\ntext = \"one day I will see the world\"\r\ncandidate_labels = ['travel', 'cooking', 'dancing']\r\nclassifier(text, candidate_labels)\r\n\r\nNLI training data of this model includes label-nli, a NLI dataset specially constructed to improve this kind of zero-shot classification.\r\n\r\n[NLI] Natural language inference pipeline\r\nfrom transformers import pipeline\r\npipe = pipeline(\"text-classification\",model=\"sileod/deberta-v3-base-tasksource-nli\")\r\npipe([dict(text='there is a cat',\r\n  text_pair='there is a black cat')]) #list of (premise,hypothesis)\r\n# [{'label': 'neutral', 'score': 0.9952911138534546}]\r\n\r\n[TA] Tasksource-adapters: 1 line access to hundreds of tasks\r\n# !pip install tasknet\r\nimport tasknet as tn\r\npipe = tn.load_pipeline('sileod/deberta-v3-base-tasksource-nli','glue/sst2') # works for 500+ tasksource tasks\r\npipe(['That movie was great !', 'Awful movie.'])\r\n# [{'label': 'positive', 'score': 0.9956}, {'label': 'negative', 'score': 0.9967}]\r\n\r\nThe list of tasks is available in model config.json. This is more efficient than ZS since it requires only one forward pass per example, but it is less flexible.\r\n\r\n[FT] Tasknet: 3 lines fine-tuning\r\n# !pip install tasknet\r\nimport tasknet as tn\r\nhparams=dict(model_name='sileod/deberta-v3-base-tasksource-nli', learning_rate=2e-5)\r\nmodel, trainer = tn.Model_Trainer([tn.AutoTask(\"glue/rte\")], hparams)\r\ntrainer.train()\r\n\r\nEvaluation\r\nThis model ranked 1st among all models with the microsoft/deberta-v3-base architecture according to the IBM model recycling evaluation. https://ibm.github.io/model-recycling/\r\n\r\nSoftware and training details\r\nThe model was trained on 600 tasks for 200k steps with a batch size of 384 and a peak learning rate of 2e-5. Training took 15 days on Nvidia A30 24GB gpu. This is the shared model with the MNLI classifier on top. Each task had a specific CLS embedding, which is dropped 10% of the time to facilitate model use without it. All multiple-choice model used the same classification layers. For classification tasks, models shared weights if their labels matched.\r\n\r\nhttps://github.com/sileod/tasksource/\r\nhttps://github.com/sileod/tasknet/\r\nTraining code: https://colab.research.google.com/drive/1iB4Oxl9_B5W3ZDzXoWJN-olUbqLBxgQS?usp=sharing", "inference_type": "huggingface"}
